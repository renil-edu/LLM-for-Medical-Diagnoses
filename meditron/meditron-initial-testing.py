# -*- coding: utf-8 -*-
"""meditron-testing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w8SJzYGmkv_7jSVqMh5wCiF7AY3wLVWi
"""

# # Install required packages first
# !pip install transformers torch bitsandbytes accelerate

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import login  # Import login function
from google.colab import userdata

# Configure for 4-bit quantization (most lightweight option)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

# Log in to your Hugging Face account
# Replace "YOUR_HUGGINGFACE_TOKEN" with your actual token
login(token=userdata.get('HF_TOKEN'))

# Load the model and tokenizer
model_name = "epfl-llm/meditron-7b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=quantization_config,
    torch_dtype=torch.float16
)

def generate_medical_response(prompt, max_length=512):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        num_return_sequences=1,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Sample medical prompt
sample_prompt = """As a medical professional, please analyze these symptoms and provide a potential diagnosis:
- Fever of 101°F for the past 2 days
- Sore throat
- Fatigue
- Mild headache
- Slight body aches

What is the most likely diagnosis and what should be the next steps?"""

# Generate response
print("Generating medical response...")
response = generate_medical_response(sample_prompt)
print("\nResponse:")
print(response)

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import login  # Import login function
from google.colab import userdata

# Configure for 4-bit quantization (most lightweight option)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

# Log in to your Hugging Face account
# Replace "YOUR_HUGGINGFACE_TOKEN" with your actual token
login(token=userdata.get('HF_TOKEN'))

# Load the model and tokenizer
model_name = "epfl-llm/meditron-7b"
tokenizer = AutoTokenizer.from_pretrained(model_name)


# Check if GPU is available before loading the model
if torch.cuda.is_available():
    print("GPU is available!")
    print(f"Using device: {torch.cuda.get_device_name(0)}")

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        quantization_config=quantization_config,
        torch_dtype=torch.float16
    )
else:
    print("GPU is not available. Please check your runtime settings. The model will be loaded on CPU and might be slow.")
    # Load the model on CPU without quantization if GPU is not available
    model = AutoModelForCausalLM.from_pretrained(model_name) # Load on CPU

def generate_medical_response(prompt, max_length=512):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        num_return_sequences=1,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Sample medical prompt
sample_prompt = """As a medical professional, please analyze these symptoms and provide a potential diagnosis:
- Fever of 101°F for the past 2 days
- Sore throat
- Fatigue
- Mild headache
- Slight body aches

What is the most likely diagnosis and what should be the next steps?"""

# Generate response
print("Generating medical response...")
response = generate_medical_response(sample_prompt)
print("\nResponse:")
print(response)

sample_prompt2 = "Explain what causes a fever"

response2 = generate_medical_response(sample_prompt2)
print("\nResponse:")
print(response2)